{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ec6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name: Soy Vitou\n",
    "#Class: ITE-A\n",
    "#Project: Animal Recognition Using Convolutional Neural Network "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0422317",
   "metadata": {},
   "source": [
    "#Library using Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b22a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pathlib\n",
    "from torch.optim import Adam\n",
    "import glob\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17b9dd3a",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing\n",
    "\n",
    "- TRANSFORMS.TOTENSOR()                  \n",
    "- MEANING :                              \n",
    "- Convert 0-255 TO 0-1, NUMPY TO TENSORS \n",
    "\n",
    "- TRANSFORMS.NORMALIZE([0.5,0.5,0.5],[0.5,0.5,0.5]) \n",
    "- MEANING :                                         \n",
    "- 0-1 TO RANGE [-1,1] , FORMULA IS (X - MEAN)/STD   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9730d040",
   "metadata": {},
   "source": [
    "### Path of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3588b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./dataset/train\"\n",
    "test_path = \"./dataset/test\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe03e3f5",
   "metadata": {},
   "source": [
    "### Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5088d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "train_data = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path, transform = transformer),\n",
    "    batch_size = 32,\n",
    "    shuffle = True,\n",
    ")\n",
    "test_data = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path, transform = transformer),\n",
    "    batch_size = 32,\n",
    "    shuffle = True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "583c3501",
   "metadata": {},
   "source": [
    "> * Checking for devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29b8b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27d26d18",
   "metadata": {},
   "source": [
    "> * Print categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26016794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cats'], ['dogs']]\n"
     ]
    }
   ],
   "source": [
    "root = pathlib.Path(train_path)\n",
    "classes = sorted([j.name] for j in root.iterdir())\n",
    "print(classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed2885f5",
   "metadata": {},
   "source": [
    "### How the CatDogClassifier model works\n",
    "* The model takes an input image of size `(256, 3, 150, 150)` and applies a series of convolutional layers to extract features from the image.\n",
    "* The first convolutional layer has 12 filters, each of which is 3x3 pixels in size. The output of the first convolutional layer is a feature map of size `(256, 12, 150, 150)`.\n",
    "* The second convolutional layer has 20 filters, each of which is 3x3 pixels in size. The output of the second convolutional layer is a feature map of size `(256, 20, 75, 75)`.\n",
    "* The third convolutional layer has 32 filters, each of which is 3x3 pixels in size. The output of the third convolutional layer is a feature map of size `(256, 32, 75, 75)`.\n",
    "* After the convolutional layers, the model applies a `max pooling` layer to reduce the size of the feature map by a factor of `2`.\n",
    "* The output of the `max pooling` layer is `flattened` and `fed` into a `fully connected` layer with `1024 `neurons.\n",
    "* The output of the fully connected layer is then fed into a final layer with `2` neurons, one for each class (`cat` or `dog`).\n",
    "\n",
    "### brief the model work\n",
    "* Input: Image of size `(256, 3, 150, 150)`\n",
    "* Output: Class prediction (`cat` or `dog`)\n",
    "### Architecture\n",
    "\n",
    "* `3` convolutional layers with `12`, `20`, and `32` filters, respectively\n",
    "\n",
    "![title](./image/conv2d.gif)\n",
    "\n",
    "* `1` max pooling layer\n",
    "\n",
    "![title](./image/MaxpoolSample2.png)\n",
    "\n",
    "* `1` fully connected layer with `1024` neurons\n",
    "\n",
    "![title](./image/full_connected.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71cb026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatDogClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(CatDogClassifier, self).__init__()\n",
    "        \n",
    "        #Output size after convolutional filter\n",
    "        #((W - F + 2P)/ S) + 1 \n",
    "      \n",
    "        \n",
    "        #Input shape = (256, 3, 150, 150)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 12, kernel_size=3, stride = 1, padding = 1)\n",
    "        #Shape = (256, 12, 150, 150)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "        #Shape = (256, 12, 150, 150)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        #Shape = (256, 12, 150, 150)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        #Reduce the image size be fector 2\n",
    "        #Shape = (256, 12, 75, 75)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(12, 20, kernel_size=3, stride = 1, padding = 1)\n",
    "        #Shape = (256, 20, 75, 75)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        #Shape = (256, 20, 75, 75)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(20, 32, kernel_size=3, stride = 1, padding = 1)\n",
    "        #Shape = (256, 32, 75, 75)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        #Shape = (256, 32, 75, 75)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        #Shape = (256, 32, 75, 75)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=32*75*75, out_features=num_classes)\n",
    "        \n",
    "    #Feed forward function\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        \n",
    "        output = self.pool(output)\n",
    "        \n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "        \n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "        \n",
    "        #Above output will be in matrix form, with shape (256, 32, 75, 75)\n",
    "        output = output.view(-1, 32*75*75)\n",
    "        \n",
    "        #fully connected layer\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e4deef7",
   "metadata": {},
   "source": [
    "### The model and criterion\n",
    "* model = CatDogClassifier model with 2 classes\n",
    "* device = GPU if available, CPU otherwise\n",
    "* criterion = Cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab3d090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatDogClassifier(len(classes)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a616e8d6",
   "metadata": {},
   "source": [
    "### Optimizer and lose function\n",
    "* optimizer = Adam optimizer with learning rate `0.001` and weight decay `0.0001`\n",
    "* lose_function = Cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b358659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "lose_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7865966b",
   "metadata": {},
   "source": [
    "### Get size of training and testing images\n",
    "* use train count and test count for calculate test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91893ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count = len(glob.glob(train_path + '/**/*.jpg'))\n",
    "test_count = len(glob.glob(test_path + '/**/*.jpg'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1948f85",
   "metadata": {},
   "source": [
    "* Print how many images in folder\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33d1ccce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559  :  399\n"
     ]
    }
   ],
   "source": [
    "print(train_count,\" : \", test_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb542ed5",
   "metadata": {},
   "source": [
    "#Models training and Saving the best models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c486a5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 => Train lose :7 =>Test Accuracy : 0.3667262969588551\n",
      "Epoch: 1 => Train lose :3 =>Test Accuracy : 0.33810375670840787\n",
      "Epoch: 2 => Train lose :1 =>Test Accuracy : 0.33810375670840787\n",
      "Epoch: 3 => Train lose :0 =>Test Accuracy : 0.5080500894454383\n",
      "Epoch: 4 => Train lose :0 =>Test Accuracy : 0.4561717352415027\n",
      "Epoch: 5 => Train lose :0 =>Test Accuracy : 0.4597495527728086\n",
      "Epoch: 6 => Train lose :0 =>Test Accuracy : 0.47942754919499103\n",
      "Epoch: 7 => Train lose :0 =>Test Accuracy : 0.47942754919499103\n",
      "Epoch: 8 => Train lose :0 =>Test Accuracy : 0.4669051878354204\n",
      "Epoch: 9 => Train lose :0 =>Test Accuracy : 0.46869409660107336\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy = 0.0\n",
    "    train_lose = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_data):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        lose = lose_function(outputs, labels)\n",
    "        lose.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_lose += lose.cpu().data*images.size(0)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        \n",
    "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
    "    \n",
    "    train_accuracy = train_accuracy /train_count\n",
    "    train_lose = train_lose / train_count\n",
    "    \n",
    "    \n",
    "    #Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(test_data):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _,prediction = torch.max(outputs.data, 1)\n",
    "        test_accuracy += int(torch.sum(prediction == labels.data))\n",
    "        \n",
    "    test_accuracy = test_accuracy /train_count\n",
    "    \n",
    "    print('Epoch: ' + str(epoch) + ' => Train lose :' + str(int(train_lose)) + ' =>Test Accuracy : ' + str(test_accuracy))\n",
    "    best_accuracy = 0.0\n",
    "    #Saving the best models\n",
    "    if test_accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_checkpoint.model')\n",
    "        best_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b07e13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
